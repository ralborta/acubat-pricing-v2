# ğŸ§  Proof-of-Life: VerificaciÃ³n de Uso de IA

## ğŸ“‹ Problema Identificado

**Tu preocupaciÃ³n era vÃ¡lida**: No habÃ­a forma visible de verificar que realmente se estaba usando la IA. Los logs eran genÃ©ricos y no habÃ­a mÃ©tricas.

## âœ… SoluciÃ³n Implementada

### 1. **Wrapper de InstrumentaciÃ³n** (`app/lib/llm.ts`)

```typescript
export async function withLLM<T>(
  fn: () => Promise<T>,
  meta: Record<string, any>
): Promise<T> {
  const t0 = Date.now();
  const requestId = `req_${Date.now()}_${Math.random().toString(36).substring(7)}`;
  
  try {
    console.log(`ğŸ§  [${requestId}] IA LLAMADA INICIADA`, { ...meta });
    const out = await fn();
    const ms = Date.now() - t0;
    console.log(`ğŸ§  [${requestId}] âœ… IA OK`, { ...meta, ms, latency_ms: ms });
    return out;
  } catch (e: any) {
    const ms = Date.now() - t0;
    console.error(`ğŸ§  [${requestId}] âŒ IA ERROR`, { ...meta, ms, error: e?.message });
    throw e;
  }
}
```

**Beneficio**: Cada llamada a IA tiene un `request_id` Ãºnico y logs claros.

---

### 2. **DiagnÃ³stico en Respuesta** (`MapColumnsOutput`)

```typescript
export interface MapColumnsOutput {
  // ... campos existentes
  __diag?: {
    request_id?: string;
    model: string;
    prompt_tokens?: number;
    completion_tokens?: number;
    latency_ms?: number;
    vendorHint?: string;
    fileName?: string;
    sheetName?: string;
  };
  __source?: "IA" | "HEURISTIC";
  __forced?: boolean;
}
```

**Beneficio**: Cada respuesta de IA incluye metadata de la llamada.

---

### 3. **InstrumentaciÃ³n en `mapColumnsStrict`**

**ANTES** (sin proof-of-life):
```typescript
response = await client.chat.completions.create(basePayload);
```

**DESPUÃ‰S** (con proof-of-life):
```typescript
response = await withLLM(async () => {
  return await client.chat.completions.create(basePayload);
}, {
  step: "mapColumnsStrict",
  model: model || LLM_MODEL,
  promptHash: promptHash.substring(0, 8),
  vendorHint: vendorHint || 'none'
});

// Extraer mÃ©tricas
requestId = response?.id || `req_${Date.now()}`;
promptTokens = response?.usage?.input_tokens || 0;
completionTokens = response?.usage?.output_tokens || 0;

// Agregar a resultado
out.__diag = {
  request_id: requestId,
  model: model || LLM_MODEL,
  prompt_tokens: promptTokens,
  completion_tokens: completionTokens,
  latency_ms: latencyMs
};
out.__source = "IA";
```

**Beneficio**: MÃ©tricas completas en cada respuesta.

---

### 4. **DiagnÃ³stico en Respuesta API**

```typescript
const diagnosticoIA: any[] = [];

// DespuÃ©s de mapColumnsStrict:
diagnosticoIA.push({
  file: file.name,
  sheet: hojaActual,
  source: result.__source || 'IA',
  forced: FORCE_IA || false,
  model: result.__diag?.model || 'gpt-4o-mini',
  request_id: result.__diag?.request_id || 'unknown',
  prompt_tokens: result.__diag?.prompt_tokens || 0,
  completion_tokens: result.__diag?.completion_tokens || 0,
  latency_ms: result.__diag?.latency_ms || 0,
  confidence: result.confianza || 0,
  tipo: result.tipo || null,
  marca: result.marca || null,
  modelo: result.modelo || null,
  precio_col: result.precio_ars || null
});

// En respuesta final:
return NextResponse.json({
  success: true,
  productos: productosProcesados,
  diagnosticoIA // ğŸ¯ VISIBLE EN RESPUESTA
});
```

---

## ğŸ¯ CÃ³mo Verificar que se Usa IA

### 1. **En los Logs del Servidor**

Busca estos logs:
```
ğŸ§  [req_1234567890_abc123] IA LLAMADA INICIADA
ğŸ§  [req_1234567890_abc123] âœ… IA OK { step: "mapColumnsStrict", model: "gpt-4o-mini", ms: 1234 }
ğŸ§  [req_1234567890_abc123] IA COMPLETADA: { model: "gpt-4o-mini", tokens: "500/200", latency_ms: 1234 }
```

**Si NO ves estos logs** â†’ **NO se estÃ¡ usando IA** (fallback activado).

---

### 2. **En la Respuesta de la API**

En el objeto `diagnosticoIA`:
```json
{
  "diagnosticoIA": [
    {
      "file": "MouraYA_Agosto_2025.xlsx",
      "sheet": "Sheet1",
      "source": "IA",              // âœ… "IA" = se usÃ³ IA
      "forced": false,
      "model": "gpt-4o-mini",      // âœ… Modelo usado
      "request_id": "req_1234...", // âœ… ID de OpenAI
      "prompt_tokens": 500,         // âœ… Tokens de entrada
      "completion_tokens": 200,    // âœ… Tokens de salida
      "latency_ms": 1234,          // âœ… Latencia en ms
      "confidence": 0.85,
      "tipo": "BATERIA",
      "precio_col": "Contado"
    }
  ]
}
```

**Si `source: "FALLBACK"`** â†’ **NO se usÃ³ IA**.

---

### 3. **Variable de Entorno para Forzar IA**

Agregar a `.env`:
```env
PRICING_FORCE_IA=1
PRICING_LLM_MODEL=gpt-4o-mini
```

Con `PRICING_FORCE_IA=1`, el sistema:
- âœ… Saltea cualquier validaciÃ³n previa
- âœ… Usa IA obligatoriamente
- âœ… Registra `forced: true` en diagnÃ³stico

---

## ğŸ“Š Logs que VerÃ¡s

### âœ… Cuando SÃ se usa IA:
```
ğŸ§  ========== USANDO DETECCIÃ“N AVANZADA CON IA ==========
ğŸ§  FORCE_IA: âŒ NO (usando normal)
ğŸ§  Modelo: gpt-4o-mini
ğŸ§  [req_1234567890_abc] IA LLAMADA INICIADA { step: "mapColumnsStrict", model: "gpt-4o-mini" }
ğŸ§  [req_1234567890_abc] âœ… IA OK { ms: 1234, latency_ms: 1234 }
ğŸ§  [req_1234567890_abc] IA COMPLETADA: { model: "gpt-4o-mini", tokens: "500/200", latency_ms: 1234, confidence: 0.85 }
ğŸ§  ========== DIAGNÃ“STICO IA ==========
ğŸ§  Source: IA
ğŸ§  Model: gpt-4o-mini
ğŸ§  Request ID: req_1234567890_abc
ğŸ§  Tokens: 500 input / 200 output
ğŸ§  Latency: 1234ms
ğŸ§  ====================================
```

### âŒ Cuando NO se usa IA (fallback):
```
âŒ Error en mapColumnsStrict: [error]
âš ï¸ Usando pickIdColumn como fallback total...
âš ï¸ MAPEO MÃNIMO CON FALLBACK: { tipo: '', modelo: '...', ... }
ğŸ§  ========== FALLBACK ACTIVADO (NO SE USÃ“ IA) ==========
```

---

## ğŸ¯ Resultado

**Antes**: No habÃ­a forma de saber si se usaba IA o no.

**DespuÃ©s**: 
- âœ… Logs claros con `request_id` Ãºnico
- âœ… MÃ©tricas de tokens y latency
- âœ… DiagnÃ³stico visible en respuesta API
- âœ… Variable para forzar IA si es necesario
- âœ… Imposible no saber si se usa IA

---

## ğŸ” CÃ³mo Verificar en el Frontend

En la respuesta de la API, busca `diagnosticoIA`:

```typescript
const response = await fetch('/api/pricing/procesar-archivo', ...);
const data = await response.json();

if (data.diagnosticoIA && data.diagnosticoIA.length > 0) {
  const diag = data.diagnosticoIA[0];
  
  if (diag.source === 'IA' && diag.model === 'gpt-4o-mini') {
    console.log('âœ… IA USADA:', {
      request_id: diag.request_id,
      tokens: `${diag.prompt_tokens}/${diag.completion_tokens}`,
      latency: `${diag.latency_ms}ms`
    });
  } else {
    console.warn('âš ï¸ IA NO USADA - Fallback activado');
  }
}
```

---

## ğŸ“ Variables de Entorno

Agregar a `.env` o `.env.local`:
```env
# Forzar uso de IA (opcional)
PRICING_FORCE_IA=1

# Modelo de IA a usar
PRICING_LLM_MODEL=gpt-4o-mini

# API Key de OpenAI (obligatorio)
OPENAI_API_KEY=sk-...
```

---

## âœ… VerificaciÃ³n RÃ¡pida

**10 segundos para verificar:**

1. Sube un archivo
2. Abre Network tab en DevTools
3. Busca la respuesta de `/api/pricing/procesar-archivo`
4. En `diagnosticoIA[0]`:
   - âœ… `source: "IA"` â†’ **SÃ se usÃ³ IA**
   - âœ… `model: "gpt-4o-mini"` â†’ **Modelo confirmado**
   - âœ… `request_id: "req_..."` â†’ **Request ID de OpenAI**
   - âœ… `prompt_tokens > 0` â†’ **Se enviaron datos a IA**
   - âŒ `source: "FALLBACK"` â†’ **NO se usÃ³ IA**

**Si todo es âœ…** â†’ **IA estÃ¡ funcionando correctamente**.

